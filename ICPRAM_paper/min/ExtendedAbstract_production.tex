\section{\uppercase{Production}}
\label{sec:production}

%K-Means is used for the production of the ensemble.
The main contribution for the optimization of the production of the ensemble is the implementation of a parallel K-Means for GPUs using NVIDIA's Compute Unified Device Architecture (CUDA).


%\subsection{K-Means}

%K-Means is a hard clustering algorithm that takes two initialization parameters: the number of clusters and the centroid initializations.
%It starts by assigning each pattern to its closer cluster based on the cluster's centroid.
%This is called the \textbf{labeling} step since one usually uses cluster labels for this assignment.
%The centroids are then recomputed based on this assignment, in the \textbf{update} step.
%The new centroids are the mean of all the patterns belonging to the clusters.
%These two steps are executed iteratively until a stopping condition is met, usually the number of iterations, a convergence criteria or both.
%The initial centroids are usually chosen randomly.

\subsection{Programming for GPUs with CUDA}

A GPU is comprised by multiprocessors.
Each multiprocessors contains several simpler cores, which execute a \emph{thread}, the basic unit of computation in CUDA.
Threads are grouped into \emph{blocks} which are part of the block \emph{grid}.
The number of threads in a block is typically higher than the number of cores in a multiprocessor, so the threads from a block are divided into smaller batches, called \emph{warps}.
The computation of one block is independent from other blocks and each block is scheduled to one multiprocessor, where several warps may be active at the same time.
CUDA employs a \emph{single-instruction multiple-thread} execution model \cite{lindholm2008nvidia}, where all threads in a block execute the same instruction at any time. %changed

GPUs have several memories, depending on the model.
Accessible to all cores are the global memory, constant memory and texture memory, of which the last two are read-only.
Blocks share a smaller but significantly faster memory called shared memory, which resides inside the multiprocessor to which all threads in a block have access to.
Finally, each thread has access to local memory, which resides in global memory space and has the same latency for read and write operations.
%However, if the thread is using only single variables or constant sized arrays, it uses register space, which is very fast.

%Typically, CUDA applications follow the same flow.
%First, the host starts by transferring any necessary data to the device memory.
%The next step is selecting the \emph{kernel} (the function that will run on the GPU processors) and the thread topology (configuration of threads in a block and blocks in the grid).
%The set-up phase is followed by the computation phase in the GPU.
%Finally, the host will transfer back the results from the device.

\subsection{Parallel K-Means}



Several parallel implementations of K-Means for the GPU exist in literature \cite{Zechner2009,Sirotkovi2012} and all report speed-ups relative to their sequential counterparts.
The implementation of the present work followed the version in \cite{Zechner2009}, which only parallelizes the labeling stage and was programmed in Python using the Numba CUDA API. % \cite{numba}
This was further motivated from empirical data which suggested that, on average, roughly $98 \%$ of the time was spent in the labeling step.
A relevant difference is that our implementation is not making use of shared memory.
Our implementation starts by transferring the data to the GPU (pattern set and initial centroids) and allocates space in the GPU for the labels and distances from patterns to their closest centroids.
Initial centroids are chosen randomly from the dataset and the dataset is only transferred once.
As shown in Algorithm \ref{alg:labelling}, each GPU thread will compute the closest centroid to each point of its corresponding set, $X_{ThreadID}$, of 2 data points (2 points per thread proved to yield the highest speed-up).
The labels and corresponding distances are stored in arrays and sent back to the host afterwards for the computation of the new centroids.
The implementation of the centroid recomputation, in the CPU, starts by counting the number of patterns attributed to each centroid.
Afterwards, it checks if there are any "empty" clusters, i.e. if there are centroids that are not the closest ones to any pattern.
Dealing with empty clusters is important because the target use expects that the output number of clusters be the same as defined in the input parameter.
Centroids corresponding to empty clusters will be the patterns that are furthest away from their centroids.
Any other centroid $c_j$ will be the mean of the patterns that were labeled $j$.

\begin{algorithm}
\SetAlgoLined
\KwIn{dataset, centroids, ThreadID}
\KwOut{labels, distances}
    \ForAll{$x_i \in X_{ThreadId}$}{
        $bestDist \leftarrow Inf$\;
        $bestLabel \leftarrow -1$\;
        \ForEach{centroid $c_j$}{
            $dist \leftarrow D(x_i, c_j)$\;
            \If{$dist < bestDist$}{
                $bestDist \leftarrow dist$\;
                $bestLabel \leftarrow j$\;
            }
        }
        $labels_i \leftarrow bestLabel$\;
        $distances_i \leftarrow bestDist$\:
        % the two lines below describe functionally what is being done but not exactly how it is being done programmatically
        %$labels_i \leftarrow arg\:min\:D(x_i,c_j)$\;
        %$distances_i \leftarrow D(x_i, c_{labels_i})$\;
    }
\caption{GPU kernel for the labeling phase.}
\label{alg:labelling}
\end{algorithm}

In the our implementation several parameters can be adjusted by the user, such as the grid topology and the number of patterns that each thread processes.