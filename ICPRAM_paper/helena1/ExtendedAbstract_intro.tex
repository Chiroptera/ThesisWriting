\section{\uppercase{Introduction}}
\label{sec:intro}

% Motivation and state-of-the-art...

Advances in technology allow for the collection and storage of unprecedented amount and variety of data, of which there is an interest in performing automated analysis for generation of knowledge and new insights.
A growing body of formal methods aiming to model, structure and/or classify data already exist. %, e.g. linear regression, principal component analysis, cluster analysis, support vector machines, neural networks.
%Cluster analysis is an interesting tool because it typically does not make assumptions on the structure of the data, which is specially interesting when no prior information about the data is known.
Of these, cluster analysis is an interesting tool, since it typically does not rely on external information about the data.
Clustering algorithms explore the structure of the data by organizing it into clusters.

A vast body of work on clustering algorithms exists \cite{Jain2010}, but usually different methods are suited to datasets of different characteristics and are not able to discover all possible cluster shapes.
Inspired by the work on sensor fusion and classifier combination, ensemble clustering approaches \cite{Fred2001,Fred2002,Strehl2002} have been proposed to address that challenge.

The present works builds on the Evidence Accumulation Clustering (EAC) framework presented by \cite{Fred2002,Fred2005}.
%cite{Fred2002,fred2002evidence,Fred2005}.
The underlying idea of EAC is to take a collection of partitions, a \emph{clustering ensemble}, and combine it into a single partition of better quality than any in the ensemble.
Accordingly, EAC tries to find an optimal partition $P^*$ containing $k^*$ clusters that is consistent with and robust to small variations in the ensemble and has a good fit with ground truth, when available.
EAC makes no assumption on the number of clusters in each partition in the ensemble.
Its approach is divided in 3 steps:

\begin{enumerate}
\item \textbf{Production} of a clustering ensemble (the evidence);
\item \textbf{Combination} of the ensemble into a co-association matrix $C$, by means of a voting mechanism based on co-occurrences in the clusters of the $N$ partitions in the ensemble:

\begin{center}
$$
C_{i,j} = \frac{no. \; co-occurences \; in \; cluster}{N}
$$
\end{center}

\item \textbf{Recovery} of the natural clusters of the data.
\end{enumerate}

% introduce the creation of the ensemble
The ensemble is usually produced by random initialization of K-Means, %, specifying only the $[K_{min}, K_{max}]$ interval from which a random number of centroids will be picked.
to have diversity with the intention of better capturing the underlying structure of the data.  That diversity can also be obtained by varying the number of clusters in the ensemble's partitions within an interval $[K_{min}, K_{max}]$.
The choice of this interval and its influence on the EAC's properties will be a topic of discussion further ahead.

%The ensemble of partitions is combined in the second step, where a non-linear transformation turns the ensemble into a co-association matrix \cite{Fred2005}, i.e. a matrix $\mathcal{C}$ where each of its elements $n_{ij}$ is the association value between the pattern pair $(i,j)$, which is computed as the number of co-occurrences in the same cluster.
%The rationale is that pairs that are frequently clustered together are more likely to be representative of a true link between the patterns \cite{Fred2002}, revealing the underlying structure of the data.
%The construction of the co-association matrix is at the very core of this method.

%The co-association matrix is then used in the final step for obtaining the final partition.
The co-association between any two patterns can be interpreted as a proximity measure.
Hierarchical algorithms such as Single-Link or Average-Link have been used, since they operate over pair-wise dissimilarity matrices.
These output a dendrogram, which codes a hierarchy of a pattern set, and can be cut at different levels to produce a partition.
% However, one must convert the original similarity values to dissimilarities.
%Furthermore, the lifetime criteria can automatically decide the number of clusters by cutting a dendrogram in the interval corresponding to the longest lifetime.
%The k-cluster lifetime is defined as the range of threshold values on the dendrogram that lead to the identification of k clusters \cite{Fred2005}.

EAC is robust, but, currently, its computational complexity restricts its application to small datasets.
The two approaches for addressing the space complexity of EAC that have been proposed are (1) exploiting the sparse nature of the co-association matrix \cite{Lourenco2010} and (2) consider only the $k$-Nearest Neighbors of each pattern when building the co-association matrix.

The goal of this work is to scale the EAC method to large datasets, using technology found on a typical workstation, by optimizing for both speed and memory usage.
We speed-up the production of ensemble by using the power of Graphics Processing Units.
The space complexity of the co-association matrix is dealt by exploiting its sparse nature, extending the work of \cite{Lourenco2010}.
We choose to use the Single-Link algorithm to speed-up the final clustering for small data and make it applicable for large datasets by using external memory algorithms.

The approaches for optimizing each of the steps are presented in sections \ref{sec:production}, \ref{sec:combination} and \ref{sec:recovery} for the production, combination and recovery steps, respectively.
The implemented optimizations are tested and the results presented in section \ref{sec:resul}.
Finally, the conclusions can be found in section \ref{sec:conc}.