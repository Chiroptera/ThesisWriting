%!TEX root = Thesis.tex

\chapter{Discussion?}
\label{chapter:discussion}

\section{real num assocs compared to samples per cluster}

\cite{Lourenco2010} reports that, on average, \"the overall contribution of the clustering ensemble (including unbalanced clusters) duplicates the co-associations produced in a single balanced clustering with Kmin clusters\".
However, the spectrum of datasets evaluated regarding cardinality was smaller than that evaluated in the present work.
The results suggest that the contribution of the ensemble is in fact higher. %TODO check if this is true because I only have the data for the maximum number of assocs and not the average one

\section{Trade-off speed accuracy memory}

%TODO
% in the Discussion or Conclusions chapter, include a reflection along the folloing lines
When a problem of clustering of big data is at hand, the user should reflect upon what the problem at hand really requires: speed or accuracy. The user should take into consideration the nature of the data and the requirements of the problem (concerning speed and accuracy) before proceeding to the execution of the analysis. The present body of work reflects a method of clustering over big data using a high accuracy, but also high cost, method. Other methods offer the opposite, low cost, low to average accuracy. 

\section{GPU MST}

The parallel version of the final step of EAC showed a slowdown relative to its sequential counterpart.
This slowdown is related with the performance of the MST algorithm.
The implementation of the algorithm was tested in some of the same graphs as those reported in \cite{Sousa2015} and revealed a speedup.
However, when using the this MST solver on the target graphs (the co-association matrices) not only there was no speedup, but a significantly slowdown was observed, reaching up to nine times slower.
The underlying reason for this is believed to be that the number of edges to node ratio of these graphs is low compared to that typically seen in co-associations matrix, even when using a prototype subset of the original matrix.
Since the parallel computation is anchored to nodes, the workload per node is higher from the beginning and can increase significantly as the algorithm progresses.
Furthermore, the workload can become highly unbalanced with some threads having to process hundreds of thousands of edges while others only a few thousands.


\section{Expanding this work to other scalability paradigms
% TODO: the idea of this section is to state the main ideas behind the scalability of EAC and see if they are transferable for other scalability paradigms, e.g. clusters
The present work focused on two main approaches for scalability: GPGPU and out-of-core solution. A current trend in computing of big data is cluster computing, which allows for distributed and parallel computing. For the sake of completeness it is interesting to discuss if the ideas related to the former two paradigms are transferable to the later.

The concept of GPGPU is one of parallelization. It is based on the fact that each computing thread in the GPU will execute instructions on a restricted subset of the entire dataset. This idea is easily transferable to cluster computing, as it is a core concept on both paradigms.
For this reason, the generation of the ensemble is a step of EAC that can be very easily transfered to a computing cluster.
