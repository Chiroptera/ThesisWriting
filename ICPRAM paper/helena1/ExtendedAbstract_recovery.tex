\section{\uppercase{Optimization of the recovery step of EAC}}
\label{sec:recovery}

\subsection{Single-Link}

Single-Link (SL) is a Hierarchical Agglomerative Clusteirng (HAC) algorithm that has been used for the last step of EAC.
HAC algorithms operate over a pair-wise dissimilarity matrix and output a dendrogram.
The main steps of an agglomerative hierarchical clustering algorithm are \cite{Jain1999} (1) the creation of a pair-wise dissimilarity matrix of all patterns, where each pattern is a distinct cluster singleton; (2) finding the closest clusters, merge them and update the matrix to reflect this change; and, (3) repeating step 2 until all patterns belong to the same cluster.
The algorithm stops when $n-1$ merges have been performed, which is when all patterns have been connected in the same cluster.
The proximity measure between clusters in the second step distinguishes between the different HAC linkage algorithms, such as Single-Link , Average-Link, Complete-Link, among others.
In SL, the proximity between any two clusters is the the dissimilarity between their closest patterns.

An interesting property of SL, which will be relevant further on, is its equivalence with a Minimum Spanning Tree (MST) \cite{Gower1969}.
In graph theory, a MST is a tree that connects all vertices together while minimizing the sum of all the distances between them.
In the context of EAC, the edges of the MST are the distances between the patterns and the vertices are the patterns themselves.
A MST contains all the information necessary to build a SL dendrogram.
One of the advantages of using an MST based clustering is that it processes only non-zero values while a typical SL algorithm will process all pair-wise proximities, even if they are null.


%Two candidates were considered for the final step of EAC.
%A SL based on a GPU version of MST \cite{Sousa2015} was implemented.
%External algorithms were the second candidate solution.
%This solution is based on storing the co-association matrix on disk, performing the expensive computation (memory and speed wise) of \emph{argsort} and then processing the matrix in batches until the final MST is extracted.

\subsection{Kruskal's algorithm and implementation}

% Kruskal algorithm
Kruskal's algorithm was used for computing the MST.
The original paper of Kruskal's \cite{kruskal1956shortest} algorithm describes three approaches for finding a MST.
The SciPy scientific computing Python library \cite{JonesSciPy} offers an efficient implementation for the following construct (taken directly from the original paper of Kruskal's algorithm):
\begin{displayquote}
CONSTRUCTION A. Perform the following step as many times as possible: Among the edges of G not yet chosen, choose the shortest edge which does not form any loops with those edges already chosen. Clearly the set of edges eventually chosen must form a spanning tree of G, and in fact it forms a shortest spanning tree.
\end{displayquote}
If the graph $G$ if connected, the algorithm will stop before processing all edges when $|V| - 1$ edges are added to the MST, where $V$ is the set of edges.
This implementation works on a sparse matrix in the CSR format.

One of the main steps of the implementation is computing the order of the edges of the graph without sorting the edges themselves, an operation called \emph{argsort}.
To illustrate this, the \emph{argsort} operation on the array $ \left [  4 , 5 , 2 , 1, 3 \right ]$ would yield $ \left [  3 , 2 , 4, 0 , 1 \right ]$ since the the smallest element is at position 3 (starting from 0), the second smalled at position 2, etc.
This operation is much less time intensive than computing the shortest edge at each iteration.
However, the total space used is typically 8 times larger for EAC since the data type of the weights uses only one byte and the number of associations is very large, forcing the use of a 8 byte integer for the \emph{argsort} output array.
%This is the real motivation to store the co-association matrix in disk and use an external sorting algorithm.
This motivated the storage of the co-association matrix in disk and usage an external sorting algorithm.

\subsection{Kruskal's implementation with an external sorting algorithm}

The \emph{PyTables} library \cite{pytables}, which is built on top of the \emph{HDF5} format \cite{hdf5}, was used for storing the co-association matrix in graph format, performing the external sorting for the \emph{argsort} operation and loading the graph in batches for processing.
This implementation starts by storing the CSR graph to disk.
However, instead of saving the \emph{indptr} array directly, it stores an expanded version of the same length as the \emph{indices} array, where the $i$-th element contains the origin vertex (or row) of the $i$-th edge.
This way, a binary search for discovering the origin vertex becomes unnecessary.

Afterwards, the \emph{argsort} operation is performed by building a completely sorted index (CSI) \cite{AltetiAbad2007} of the \emph{data} array of the CSR matrix.
It should be noted that the arrays themselves are not sorted.
Instead, the CSI allows for a fast indexing of the arrays in a sorted manner (according to the order of the edges).
The process of building the CSI has a very low main memory usage and can be disregarded in comparison to the co-association matrix.

The SciPy implementation of Kruskal's algorithm was modified to work with batches of the graph.
This was easily implemented just by making the additional data structures used in the building of MST persistent between iterations.
The new implementation loads the graph in batches and in a sorted manner, e.g. first load a batch of the 1000 shortest edges, then a batch of the next 1000 shortest edges, etc.
Each batch must be processed sequentially since the edges must be processed in a sorted manner, which means there is no possibility for parallelism in this process.
Typically, the batch size is a very small fraction of the size of the edges, so the total memory usage for building the MST is overshadowed by the size of the co-association matrix.
The time complexity for building the CSI is higher than that of computing the \emph{argsort} operation, but the formal time complexity is not reported in the source \cite{AltetiAbad2007}.
As an example, for a $500 \: 000$ vertex graph the SL-MST approach took $54.9$ seconds while the external memory approach took $2613.5$ seconds - 2 orders of magnitude higher.